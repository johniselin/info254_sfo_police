{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feather\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# others\n",
    "import os\n",
    "import random as rn\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rnn_data(path, window, predict_ts, isdim3=True, geo_col=[\"geoid10_tract\"], y_cols=[\"crime\"]):\n",
    "    \"\"\"\n",
    "    y_cols: [\"crime\"] or [\"incident_type_0\", \"incident_type_1\", \"incident_type_2\"]\n",
    "    geo_col: [\"geoid10_tract\"] or [\"geoid10_block\"]\n",
    "    return y_all and x_all of given path\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    df = feather.read_dataframe(path)\n",
    "    df.sort_values(by=[\"datetime\", \"geoid10_tract\"], inplace=True)\n",
    "    df.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "    # input columns\n",
    "    x_cols = list(df.drop(y_cols + geo_col, axis=1).columns)\n",
    "\n",
    "    # group by geoid\n",
    "    geo_grs = df.groupby(by=geo_col)\n",
    "\n",
    "    # arrayes to store x and y\n",
    "    # (no of timesteps, window size, no of tracts,  no of features, )\n",
    "    n_timesteps = int(len(df) / len(geo_grs)) - window - predict_ts + 1\n",
    "    x_all = np.empty(shape=(n_timesteps, window, len(geo_grs), len(x_cols + y_cols)))\n",
    "\n",
    "    # (output size, no of tracts, no of outputs)\n",
    "    y_all = np.empty(shape=(n_timesteps, len(geo_grs), len(y_cols)))\n",
    "\n",
    "    # to store geo_ids and y_all's datetime\n",
    "    geo_ids = []\n",
    "\n",
    "    y_datetime = df.index.unique()[window + predict_ts - 1:]\n",
    "\n",
    "    for i, (geo_id, gr) in enumerate(tqdm(geo_grs)):\n",
    "        geo_ids.append(geo_id)\n",
    "        x_values = gr[y_cols + x_cols].values\n",
    "        y_values = gr[y_cols].values\n",
    "\n",
    "        for j in range(window, len(gr) - predict_ts + 1):\n",
    "            # generate x_all\n",
    "            x_all[j - window, :, i, :] = x_values[j - window:j, :]\n",
    "            y_all[j - window, i, :] = y_values[j + predict_ts - 1, :]\n",
    "\n",
    "    if isdim3:\n",
    "        x_all = np.reshape(x_all,\n",
    "                           newshape=(x_all.shape[0], x_all.shape[1], x_all.shape[2] * x_all.shape[3]))\n",
    "        y_all = np.reshape(y_all,\n",
    "                           newshape=(y_all.shape[0], y_all.shape[1] * y_all.shape[2]))\n",
    "\n",
    "    return x_all, y_all, geo_ids, y_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set configuration\n",
    "path = \"./features/features_binary_tract_2H.feather\"\n",
    "window = 12\n",
    "predict_ts = 1  # how many timesteps future does the model predict? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load x and y\n",
    "x_all, y_all, geo_ids, y_datetime = load_rnn_data(path=path,\n",
    "                                                  window=window,\n",
    "                                                  predict_ts=predict_ts,\n",
    "                                                  isdim3=True,\n",
    "                                                  geo_col=[\"geoid10_tract\"],\n",
    "                                                  y_cols=[\"crime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_all.shape)\n",
    "print(y_all.shape)\n",
    "print(len(geo_ids))  # to convert model output later\n",
    "print(len(y_datetime))  # to convert model output later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scaling\n",
    "# scaler = MinMaxScaler()\n",
    "# x_all = scaler.fit_transform(x_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(x_all, y_all, n_splits=5, model=None, fit_params=None, baseline=False):\n",
    "    \"\"\"\n",
    "    :param baseline: True or False (defualt: False)\n",
    "    :return: train and test scores and prediction of y on test data\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare dictionary to store scores\n",
    "    train_scores = {}\n",
    "    metrics = [\"acc\", \"log_loss\"]\n",
    "    for metric in metrics:\n",
    "        train_scores[metric] = []\n",
    "    test_scores = deepcopy(train_scores)\n",
    "\n",
    "    # prepare dictionary to store predictions\n",
    "    y_test_probs = np.zeros_like(y_all)\n",
    "\n",
    "    # time series split\n",
    "    tss = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    for split, (train_idx, test_idx) in enumerate(tss.split(x_all, y_all)):\n",
    "\n",
    "        print(\"---------- split {0} ----------\".format(split))\n",
    "        print(\"[{0:%H:%M:%S}] train_index:{1}~{2} test_index:{3}~{4}\".format(\n",
    "            datetime.now(), train_idx[0], train_idx[-1], test_idx[0], test_idx[-1]))\n",
    "\n",
    "        # create train and test set\n",
    "        x_train = x_all[:train_idx[-1]]\n",
    "        y_train = y_all[:train_idx[-1]]\n",
    "        x_test = x_all[test_idx[0]:test_idx[-1]]\n",
    "        y_test = y_all[test_idx[0]:test_idx[-1]]\n",
    "\n",
    "        if baseline:\n",
    "            # return 0 for all predicted probabiliby\n",
    "            y_train_prob = np.zeros_like(y_train)\n",
    "            y_test_prob = np.zeros_like(y_test)\n",
    "\n",
    "            # return 0 for all binary predictions\n",
    "            y_train_pred = np.zeros_like(y_train)\n",
    "            y_test_pred = np.zeros_like(y_test)\n",
    "\n",
    "        else:            \n",
    "            # train\n",
    "            model.fit(x_train, y_train, **fit_params)\n",
    "\n",
    "            # predict\n",
    "            y_train_prob = model.predict(x_train)\n",
    "            y_test_prob = model.predict(x_test)\n",
    "\n",
    "            # convert form probability to binary\n",
    "            y_train_pred = np.fix(y_train_prob)\n",
    "            y_test_pred = np.fix(y_test_prob)\n",
    "\n",
    "        # store test prediction\n",
    "        y_test_probs[test_idx[0]:test_idx[-1]] = y_test_prob\n",
    "\n",
    "        # calculate metrics\n",
    "        train_log_loss = log_loss(y_train.flatten(), y_train_prob.flatten())\n",
    "        test_log_loss = log_loss(y_test.flatten(), y_test_prob.flatten())\n",
    "        train_acc = accuracy_score(y_train.flatten(), y_train_pred.flatten())\n",
    "        test_acc = accuracy_score(y_test.flatten(), y_test_pred.flatten())\n",
    "\n",
    "        # store scores\n",
    "        train_scores[\"log_loss\"].append(train_log_loss)\n",
    "        test_scores[\"log_loss\"].append(test_log_loss)\n",
    "        train_scores[\"acc\"].append(train_acc)\n",
    "        test_scores[\"acc\"].append(test_acc)\n",
    "\n",
    "        print(\"[{0:%H:%M:%S}] train_log_loss:{1} test_log_loss:{2}\".format(\n",
    "            datetime.now(), train_log_loss, test_log_loss))\n",
    "        print(\"[{0:%H:%M:%S}] train_acc:{1} test_acc:{2}\\n\".format(\n",
    "            datetime.now(), train_acc, test_acc))\n",
    "\n",
    "        # convert to dataframe\n",
    "        train_scores_df = pd.DataFrame(train_scores)\n",
    "        test_scores_df = pd.DataFrame(test_scores)\n",
    "\n",
    "    return train_scores_df, test_scores_df, y_test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM():\n",
    "    \"\"\"\n",
    "    class to input into function time_series_cv\n",
    "    \"\"\"\n",
    "    def __init__(self, units=100,\n",
    "                 dropout_rate=0.2, activation=\"sigmoid\",\n",
    "                 optimizer=\"adam\", loss=\"binary_crossentropy\"):\n",
    "        \"\"\"\n",
    "        define LSTM model with given arguments\n",
    "        \"\"\"\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs=200, early_stopping_patience=5,\n",
    "            batch_size=128, validation_split=0.1, train_shuffle=False):\n",
    "        \"\"\"\n",
    "        fit train data\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(units=self.units, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "        self.model.add(Dropout(self.dropout_rate))\n",
    "        self.model.add(Dense(units=y_train.shape[1], activation=self.activation))\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss, metrics=[\"acc\"])\n",
    "\n",
    "        earlystopping = EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience)\n",
    "        self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                       validation_split=validation_split, shuffle=train_shuffle,\n",
    "                       callbacks=[earlystopping])\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        \"\"\"\n",
    "        return predicted values of x_test\n",
    "        \"\"\"\n",
    "        y_prob = self.model.predict(x_test)\n",
    "        return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters at model definition\n",
    "def_params = {\n",
    "    \"units\":100,\n",
    "    \"dropout_rate\":0.159961594635,\n",
    "    \"activation\":\"sigmoid\",\n",
    "    \"optimizer\":Adam(lr=1.77547438185e-05),\n",
    "    \"loss\":\"binary_crossentropy\"\n",
    "}\n",
    "\n",
    "# parameters at model training\n",
    "fit_params = {\n",
    "    \"epochs\":500,\n",
    "    \"early_stopping_patience\":5,\n",
    "    \"batch_size\":128,\n",
    "    \"validation_split\":0.1,\n",
    "    \"train_shuffle\":False\n",
    "}\n",
    "\n",
    "# number of time series cv splits\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(0)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "rn.seed(0)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(0)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# define model\n",
    "model = MyLSTM(**def_params)\n",
    "\n",
    "# time series cross validation\n",
    "train_scores_df, test_scores_df, y_test_probs = time_series_cv(x_all,\n",
    "                                                               y_all,\n",
    "                                                               n_splits=n_splits,\n",
    "                                                               model=model,\n",
    "                                                               fit_params=fit_params,\n",
    "                                                               baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train metrics\n",
    "train_scores_df.to_csv(\"./results/train_scores.csv\", index=False)\n",
    "train_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test metrics\n",
    "test_scores_df.to_csv(\"./results/test_scores.csv\", index=False)\n",
    "test_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probability of crime occurance\n",
    "y_probs_df = pd.DataFrame(y_test_probs, index=y_datetime, columns=geo_ids)\n",
    "y_probs_df = y_probs_df.loc[(y_probs_df != 0).any(axis=1), :]\n",
    "y_probs_df.to_csv(\"./results/y_probs.csv\", index=True)\n",
    "y_probs_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
